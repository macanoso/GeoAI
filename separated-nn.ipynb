{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Semantic Segmentation**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import TensorBoard\nimport pathlib\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-05-04T01:56:16.250427Z","iopub.execute_input":"2023-05-04T01:56:16.251061Z","iopub.status.idle":"2023-05-04T01:56:24.319661Z","shell.execute_reply.started":"2023-05-04T01:56:16.251026Z","shell.execute_reply":"2023-05-04T01:56:24.318543Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Full Pipeline**","metadata":{}},{"cell_type":"markdown","source":"## **Loss Function**","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport tensorflow.keras.backend as K\n\nclass LossFunctions:\n    @staticmethod\n    def tversky(y_true, y_pred):\n        y_true_pos = K.flatten(y_true)\n        y_pred_pos = K.flatten(y_pred)\n        true_pos = K.sum(y_true_pos * y_pred_pos)\n        false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n        false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n        alpha = 0.7\n        smooth = 1e-5\n        return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\n    @staticmethod\n    def tversky_loss(y_true, y_pred):\n        return 1 - LossFunctions.tversky(y_true,y_pred)\n\ndef categorical_focal_loss(gamma=2.0, alpha_val=None):\n    def focal_loss(y_true, y_pred):\n        \"\"\"\n        Categorical focal loss function.\n        :param y_true: one-hot encoded true labels\n        :param y_pred: predicted label probabilities\n        :param gamma:\n        :param alpha:\n        :return:\n        \"\"\"\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n\n        cross_entropy = -y_true * K.log(y_pred)\n\n        if alpha_val is not None:\n            alpha = K.constant(alpha_val)\n            class_weights = K.sum(alpha * y_true, axis=-1)\n            class_weights = K.expand_dims(class_weights, axis=-1)\n            focal_weights = class_weights * K.pow(1 - y_pred, gamma)\n            loss = focal_weights * cross_entropy\n        else:\n            focal_weights = K.pow(1 - y_pred, gamma)\n            loss = focal_weights * cross_entropy\n\n        return K.mean(loss)\n\n    return focal_loss\n\ndef binary_focal_loss(gamma=2.0, alpha=0.25):\n    def focal_loss(y_true, y_pred):\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n        pos_weight = alpha * y_true + (1 - alpha) * (1 - y_true)\n        focal_weights = pos_weight * K.pow(1 - y_pred, gamma)\n        loss = focal_weights * K.binary_crossentropy(y_true, y_pred)\n        return K.mean(loss)\n    return focal_loss\n\ndef weighted_binary_crossentropy(pos_weight=0.25):\n    def weighted_bce(y_true, y_pred):\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n        loss = pos_weight * y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred)\n        return -K.mean(loss)\n    return weighted_bce","metadata":{"execution":{"iopub.status.busy":"2023-05-04T01:56:24.321961Z","iopub.execute_input":"2023-05-04T01:56:24.322758Z","iopub.status.idle":"2023-05-04T01:56:24.476106Z","shell.execute_reply.started":"2023-05-04T01:56:24.322718Z","shell.execute_reply":"2023-05-04T01:56:24.475094Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## **Creating new folders for only binary**","metadata":{}},{"cell_type":"markdown","source":"In this code we create two new folders, one for only mask vegetation and one for only mask power line.","metadata":{}},{"cell_type":"code","source":"def create_binary_veg_mask(path):\n    folder_path = f\"{path}/MASK\"\n    # Crear una nueva carpeta para las imágenes binarias\n    dataset_name = os.path.basename(path)\n    binary_folder_path = f\"/kaggle/working/{dataset_name}/MASK_NEW_VEGETATION\"\n\n    if not os.path.exists(binary_folder_path):\n        os.makedirs(binary_folder_path, exist_ok=True)\n\n    # Leer cada archivo en la carpeta y procesar\n    for filename in os.listdir(folder_path):\n      # Cargar la imagen y convertir a escala de grises\n      img = cv2.imread(os.path.join(folder_path, filename))\n      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n      # Aplicar la umbralización\n      _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n\n      # Convertir los píxeles verde (vegetación) en blanco y los píxeles grises (líneas eléctricas) en negro\n      thresh[(img[:, :, 1] == 255) & (img[:, :, 0] == 0)] = 255\n      thresh[(img[:, :, 0] == img[:, :, 1]) & (img[:, :, 0] == img[:, :, 2]) & (img[:, :, 0] != 0)] = 0\n\n      # Guardar la imagen binaria en la nueva carpeta\n      cv2.imwrite(os.path.join(binary_folder_path, filename), thresh)\n\ndef create_binary_powerline_mask(path):\n    folder_path = f\"{path}/MASK\"\n    # Crear una nueva carpeta para las imágenes binarias\n    dataset_name = os.path.basename(path)\n    binary_folder_path = f\"/kaggle/working/{dataset_name}/MASK_NEW_POWERLINE\"\n    \n    if not os.path.exists(binary_folder_path):\n        os.makedirs(binary_folder_path, exist_ok=True)\n    \n    # Leer cada archivo en la carpeta y procesar\n    for filename in os.listdir(folder_path):\n      # Cargar la imagen y convertir a escala de grises\n      img = cv2.imread(os.path.join(folder_path, filename))\n      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n      # Aplicar la umbralización\n      _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n\n      # Convertir los píxeles (110, 110, 110) en blanco y todo lo demás en negro\n      thresh[(img[:, :, 0] == 110) & (img[:, :, 1] == 110) & (img[:, :, 2] == 110)] = 255\n      thresh[(img[:, :, 0] != 110) | (img[:, :, 1] != 110) | (img[:, :, 2] != 110)] = 0\n\n      # Guardar la imagen binaria en la nueva carpeta\n      cv2.imwrite(os.path.join(binary_folder_path, filename), thresh)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-04T01:56:24.478290Z","iopub.execute_input":"2023-05-04T01:56:24.479092Z","iopub.status.idle":"2023-05-04T01:56:24.493228Z","shell.execute_reply.started":"2023-05-04T01:56:24.479050Z","shell.execute_reply":"2023-05-04T01:56:24.492139Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## **Creation of dataset for training**","metadata":{}},{"cell_type":"code","source":"def load_and_preprocess_image(image_path, mask_path):\n        # cargar la imagen y máscara\n        image = tf.io.decode_png(tf.io.read_file(image_path))\n        mask = tf.io.decode_png(tf.io.read_file(mask_path), channels=1)\n\n        # normalizar la imagen y máscara\n        image = tf.cast(image, tf.float32) / 255.0\n        mask = tf.cast(mask, tf.float32) / 255.0\n\n        return image, mask\n    \ndef dataset_creation(path, class_train, batch_size):\n    BATCH_SIZE = batch_size\n    BUFFER_SIZE = 100\n    \n    data_dir = pathlib.Path(f'{path}/')\n    images_dir = data_dir / 'RGB'\n    dataset_name = os.path.basename(path)\n    data_mask_dir = pathlib.Path(f\"/kaggle/working/{dataset_name}/\")\n    \n    if class_train=='vegetation':\n        masks_dir = data_mask_dir / \"MASK_NEW_VEGETATION\"\n    \n    elif class_train=='powerline':\n        masks_dir = data_mask_dir / \"MASK_NEW_POWERLINE\"\n    \n    else:\n        print(\"Class doesn't exists\")\n\n    # obtener una lista de rutas de archivo para imágenes y máscaras\n    image_paths = sorted([str(path) for path in images_dir.glob('*.png')])\n    mask_paths = sorted([str(path) for path in masks_dir.glob('*.png')])\n\n    # crear un dataset a partir de las rutas de archivo\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n    \n    # aplicar la función de carga y preprocesamiento a cada par de rutas de archivo en el dataset\n    dataset = dataset.map(load_and_preprocess_image)\n\n    # dividir el dataset en conjuntos de entrenamiento, validación y prueba\n    total_samples = len(image_paths)\n    train_size = int(0.8 * total_samples)\n    val_size = int(0.1 * total_samples)\n    test_size = total_samples - train_size - val_size\n\n    train_dataset = dataset.take(train_size)\n    val_dataset = dataset.skip(train_size).take(val_size)\n    test_dataset = dataset.skip(train_size + val_size).take(test_size)\n\n    train_batches = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n    train_batches = train_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    validation_batches = test_dataset.batch(BATCH_SIZE)\n    test_batches = test_dataset.batch(BATCH_SIZE)\n    \n    return train_batches,validation_batches, test_batches\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-04T01:56:24.496401Z","iopub.execute_input":"2023-05-04T01:56:24.496812Z","iopub.status.idle":"2023-05-04T01:56:24.511480Z","shell.execute_reply.started":"2023-05-04T01:56:24.496768Z","shell.execute_reply":"2023-05-04T01:56:24.510322Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## **Creation of VGG UNet**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom tensorflow.keras.layers import Conv2D, Input, MaxPooling2D, Dropout, concatenate, UpSampling2D\nfrom tensorflow.keras.models import load_model, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras import backend as K\n\n  \nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.callbacks import *\n\nimport random \n\ndef VGGUnet(image_size):\n    vgg_weight_path = None\n    inputs = Input((image_size, image_size, 3))\n    # Block 1\n    x = Conv2D(64, (3, 3), padding='same', name='block1_conv1')(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(64, (3, 3), padding='same', name='block1_conv2')(x)\n    x = BatchNormalization()(x)\n    block_1_out = Activation('relu')(x)\n\n    x = MaxPooling2D()(block_1_out)\n\n    # Block 2\n    x = Conv2D(128, (3, 3), padding='same', name='block2_conv1')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(128, (3, 3), padding='same', name='block2_conv2')(x)\n    x = BatchNormalization()(x)\n    block_2_out = Activation('relu')(x)\n\n    x = MaxPooling2D()(block_2_out)\n\n    # Block 3\n    x = Conv2D(256, (3, 3), padding='same', name='block3_conv1')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(256, (3, 3), padding='same', name='block3_conv2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(256, (3, 3), padding='same', name='block3_conv3')(x)\n    x = BatchNormalization()(x)\n    block_3_out = Activation('relu')(x)\n\n    x = MaxPooling2D()(block_3_out)\n\n    # Block 4\n    x = Conv2D(512, (3, 3), padding='same', name='block4_conv1')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(512, (3, 3), padding='same', name='block4_conv2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(512, (3, 3), padding='same', name='block4_conv3')(x)\n    x = BatchNormalization()(x)\n    block_4_out = Activation('relu')(x)\n\n    x = MaxPooling2D()(block_4_out)\n\n    # Block 5\n    x = Conv2D(512, (3, 3), padding='same', name='block5_conv1')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(512, (3, 3), padding='same', name='block5_conv2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(512, (3, 3), padding='same', name='block5_conv3')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    for_pretrained_weight = MaxPooling2D()(x)\n\n    # Load pretrained weights.\n    if vgg_weight_path is not None:\n        vgg16 = Model(inputs, for_pretrained_weight)\n        vgg16.load_weights(vgg_weight_path, by_name=True)\n\n    # UP 1\n    x = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = concatenate([x, block_4_out])\n    x = Conv2D(512, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(512, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # UP 2\n    x = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = concatenate([x, block_3_out])\n    x = Conv2D(256, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(256, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # UP 3\n    x = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = concatenate([x, block_2_out])\n    x = Conv2D(128, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(128, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # UP 4\n    x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = concatenate([x, block_1_out])\n    x = Conv2D(64, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(64, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(1, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n\n    outputs = Activation('sigmoid')(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-04T01:56:24.513355Z","iopub.execute_input":"2023-05-04T01:56:24.513987Z","iopub.status.idle":"2023-05-04T01:56:24.546425Z","shell.execute_reply.started":"2023-05-04T01:56:24.513949Z","shell.execute_reply":"2023-05-04T01:56:24.545163Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## **Function for run models according to path**","metadata":{}},{"cell_type":"code","source":"def run_models(path,model, loss_function, num_epochs, batch_size):      \n    #load datasets\n    total_samples = len(os.listdir(f'{path}/MASK'))\n    train_size = int(0.8 * total_samples)\n    val_size = int(0.1 * total_samples)\n    test_size = total_samples - train_size - val_size\n\n    NUM_EPOCHS = num_epochs\n    BATCH_SIZE = batch_size\n    STEPS_PER_EPOCH = total_samples // BATCH_SIZE\n    VAL_SUBSPLITS = 5\n    VALIDATION_STEPS = test_size // BATCH_SIZE // VAL_SUBSPLITS\n    dataset_name = os.path.basename(path)\n    model_folder = '/kaggle/working/models/'\n    metrics_folder = '/kaggle/working/metrics/'\n    \n    if not os.path.exists(model_folder):\n        os.makedirs(model_folder, exist_ok=True)\n    \n    if not os.path.exists(metrics_folder):\n        os.makedirs(metrics_folder, exist_ok=True)\n    \n    callbacks_veg = [\n                EarlyStopping(patience=10, verbose=1),\n                ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n                ModelCheckpoint(f'/kaggle/working/models/{dataset_name}_{model_name}_{loss_name}_veg.h5', verbose=1, save_best_only=True, save_weights_only=False),\n                CSVLogger(f'/kaggle/working/metrics/{dataset_name}_{model_name}_{loss_name}_veg.csv'),\n                TensorBoard(log_dir='./logs')\n                ]\n    \n    callbacks_power = [\n                EarlyStopping(patience=10, verbose=1),\n                ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1),\n                ModelCheckpoint(f'/kaggle/working/models/{dataset_name}_{model_name}_{loss_name}_power.h5', verbose=1, save_best_only=True, save_weights_only=False),\n                CSVLogger(f'/kaggle/working/metrics/{dataset_name}_{model_name}_{loss_name}_power.csv'),\n                TensorBoard(log_dir='./logs')\n                ]\n\n    train_batches_veg, validation_batches_veg, test_batches_veg = dataset_creation(path, class_train = 'vegetation', batch_size = 8)\n    train_batches_power, validation_batches_power, test_batches_power = dataset_creation(path, class_train = 'powerline', batch_size = 8)\n    \n    #load metrics\n    metrics = [\"accuracy\", \n           tf.keras.metrics.AUC(), \n           tf.keras.metrics.SensitivityAtSpecificity(0.5), \n           tf.keras.metrics.SpecificityAtSensitivity(0.5)]\n    \n    model_2 = model\n    \n    model_veg = model.compile(optimizer=Adam(), loss=loss_function, metrics=metrics)\n    model_power = model_2.compile(optimizer=Adam(), loss=loss_function, metrics=metrics)\n    \n    model_history_veg = model.fit(train_batches_veg,\n                               epochs=NUM_EPOCHS,\n                               steps_per_epoch=STEPS_PER_EPOCH,\n                               validation_steps=VALIDATION_STEPS,\n                               validation_data=validation_batches_veg,\n                               callbacks=callbacks_veg)\n    \n    model_history_power = model_2.fit(train_batches_power,\n                               epochs=NUM_EPOCHS,\n                               steps_per_epoch=STEPS_PER_EPOCH,\n                               validation_steps=VALIDATION_STEPS,\n                               validation_data=validation_batches_power,\n                               callbacks=callbacks_power)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-04T04:54:45.003078Z","iopub.execute_input":"2023-05-04T04:54:45.003668Z","iopub.status.idle":"2023-05-04T04:54:45.035307Z","shell.execute_reply.started":"2023-05-04T04:54:45.003618Z","shell.execute_reply":"2023-05-04T04:54:45.033720Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## **Metrics**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score, jaccard_score\n\n    \ndef metrics_models(model, type_prediction, test_batches):    \n    \"\"\" Prediction & Evaluation \"\"\"\n    SCORE = []\n    threshold = 0.3\n    num_classes = 2\n    \n    if type_prediction=='vegetation':\n        classes = [\n            \"background\", \"vegetation\"\n        ]\n    elif type_prediction=='power_line':\n        classes = [\n            \"background\", \"power_line\"\n        ]\n\n    for image, mask in test_batches:\n        pred_mask = model.predict(image)\n        pred_mask = tf.cast(pred_mask, dtype=tf.float32)  # convertir a float\n        pred = tf.where(pred_mask > threshold, 1.0, 0.0)\n\n        labels = list(np.unique(pred))\n        flat_mask = tf.reshape(mask,[-1])\n        flat_pred = tf.reshape(pred, [-1])\n\n        mask_arr = flat_mask.numpy().flatten()\n        pred_arr = flat_pred.numpy().flatten()\n\n\n        \"\"\" Calculating the metrics values \"\"\"\n        f1_value = f1_score(mask_arr, pred_arr, labels=labels, average=None, zero_division=0)\n        jac_value = jaccard_score(mask_arr, pred_arr, labels=labels, average=None, zero_division=0)\n\n        SCORE.append([f1_value, jac_value])\n\n    score = np.array(SCORE)\n    score = np.mean(score, axis=0)\n\n    l = [\"Class\", \"F1\", \"Jaccard\"]\n    print(f\"{l[0]:15s} {l[1]:10s} {l[2]:10s}\")\n    print(\"-\"*35)\n\n    for i in range(num_classes):\n        class_name = classes[i]\n        f1 = score[0, i]\n        jac = score[1, i]\n        dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\n        print(dstr)\n\n    print(\"-\"*35)\n    class_mean = np.mean(score, axis=-1)\n    class_name = \"Mean\"\n\n    f1 = class_mean[0]\n    jac = class_mean[1]\n\n    dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\n    print(dstr)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T01:56:24.565885Z","iopub.execute_input":"2023-05-04T01:56:24.566474Z","iopub.status.idle":"2023-05-04T01:56:25.053325Z","shell.execute_reply.started":"2023-05-04T01:56:24.566436Z","shell.execute_reply":"2023-05-04T01:56:25.052252Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## **Visualization of predictions**","metadata":{}},{"cell_type":"code","source":"def create_mask(pred_mask, threshold=0.25):\n  pred_mask = tf.cast(pred_mask, dtype=tf.float32)  # convertir a float\n  pred_mask = tf.where(pred_mask > threshold, 1.0, 0.0)  # usar float para el threshold\n  return pred_mask[0]\n\ndef show_predictions(dataset, model, num):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])","metadata":{"execution":{"iopub.status.busy":"2023-05-04T05:13:45.855315Z","iopub.execute_input":"2023-05-04T05:13:45.856134Z","iopub.status.idle":"2023-05-04T05:13:45.864769Z","shell.execute_reply.started":"2023-05-04T05:13:45.856097Z","shell.execute_reply":"2023-05-04T05:13:45.862884Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## **First run**","metadata":{}},{"cell_type":"code","source":"image_size = 256\ntversky_loss_fn = LossFunctions.tversky_loss\n\nlist_folders = ['/kaggle/input/tesellated-without-augmentation/TESELLATED_WITHOUT_AUGMENTATION',\n               '/kaggle/input/tesellated-with-geometric-augmentation/TESELLATED_WITH_GEOMETRIC_AUGMENTATION',\n               '/kaggle/input/tesellated-with-spectral-augmentation/TESELLATED_WITH_SPECTRAL_AUGMENTATION']\nmodels_dict = {\n    'vgg': VGGUnet(image_size),\n    #'unet': Unet(image_size),\n    #'resunet': ResUnet(image_size)\n}\n\nloss_dict = {'tversky': tversky_loss_fn}","metadata":{"execution":{"iopub.status.busy":"2023-05-04T01:56:47.218687Z","iopub.execute_input":"2023-05-04T01:56:47.219102Z","iopub.status.idle":"2023-05-04T01:56:51.228129Z","shell.execute_reply.started":"2023-05-04T01:56:47.219066Z","shell.execute_reply":"2023-05-04T01:56:51.227127Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for path in list_folders:\n    create_binary_veg_mask(path)\n    create_binary_powerline_mask(path)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T01:56:52.544893Z","iopub.execute_input":"2023-05-04T01:56:52.545601Z","iopub.status.idle":"2023-05-04T01:57:46.091954Z","shell.execute_reply.started":"2023-05-04T01:56:52.545562Z","shell.execute_reply":"2023-05-04T01:57:46.090863Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Register custom loss function\ncustom_objects = {'tversky_loss': LossFunctions.tversky_loss}","metadata":{"execution":{"iopub.status.busy":"2023-05-04T01:57:46.093937Z","iopub.execute_input":"2023-05-04T01:57:46.094423Z","iopub.status.idle":"2023-05-04T01:57:46.100926Z","shell.execute_reply.started":"2023-05-04T01:57:46.094387Z","shell.execute_reply":"2023-05-04T01:57:46.099606Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### **tesellated-with-geometric-augmentation**","metadata":{}},{"cell_type":"code","source":"models_dict = {'vgg': VGGUnet(image_size)}\nloss_dict = {'tversky': tversky_loss_fn}\npath = '/kaggle/input/tesellated-with-geometric-augmentation/TESELLATED_WITH_GEOMETRIC_AUGMENTATION'\nloss = LossFunctions.tversky_loss\nepochs = 1\nbatch_size = 16\n\nfor model_name, model in models_dict.items():\n    for loss_name, loss in loss_dict.items():\n            run_models(path,model, loss, epochs, batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T04:54:59.649527Z","iopub.execute_input":"2023-05-04T04:54:59.650128Z","iopub.status.idle":"2023-05-04T04:59:53.390062Z","shell.execute_reply.started":"2023-05-04T04:54:59.650092Z","shell.execute_reply":"2023-05-04T04:59:53.388957Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"232/232 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.8166 - auc_1: 0.8807 - sensitivity_at_specificity_1: 0.9619 - specificity_at_sensitivity_1: 0.9244\nEpoch 1: val_loss improved from inf to 0.97761, saving model to /kaggle/working/models/TESELLATED_WITH_GEOMETRIC_AUGMENTATION_vgg_tversky_veg.h5\n232/232 [==============================] - 150s 571ms/step - loss: 0.3282 - accuracy: 0.8166 - auc_1: 0.8807 - sensitivity_at_specificity_1: 0.9619 - specificity_at_sensitivity_1: 0.9244 - val_loss: 0.9776 - val_accuracy: 0.3092 - val_auc_1: 0.4679 - val_sensitivity_at_specificity_1: 0.3202 - val_specificity_at_sensitivity_1: 0.2041 - lr: 0.0010\n232/232 [==============================] - ETA: 0s - loss: 0.8247 - accuracy: 0.6665 - auc_1: 0.8829 - sensitivity_at_specificity_1: 0.9463 - specificity_at_sensitivity_1: 0.9581\nEpoch 1: val_loss improved from inf to 0.91161, saving model to /kaggle/working/models/TESELLATED_WITH_GEOMETRIC_AUGMENTATION_vgg_tversky_power.h5\n232/232 [==============================] - 135s 581ms/step - loss: 0.8247 - accuracy: 0.6665 - auc_1: 0.8829 - sensitivity_at_specificity_1: 0.9463 - specificity_at_sensitivity_1: 0.9581 - val_loss: 0.9116 - val_accuracy: 0.9606 - val_auc_1: 0.5256 - val_sensitivity_at_specificity_1: 0.4045 - val_specificity_at_sensitivity_1: 0.3708 - lr: 0.0010\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Load models to predict","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential, load_model\n\n\ndataset_name = 'TESELLATED_WITH_GEOMETRIC_AUGMENTATION'\nmodel_name = 'vgg'\nloss_name = 'tversky'\n\n# Load model with registered custom objects\nmodel_veg = tf.keras.models.load_model(f'/kaggle/working/models/{dataset_name}_{model_name}_{loss_name}_veg.h5', custom_objects=custom_objects)\nmodel_power = tf.keras.models.load_model(f'/kaggle/working/models/{dataset_name}_{model_name}_{loss_name}_power.h5', custom_objects=custom_objects)\n\ntrain_batches_veg, validation_batches_veg, test_batches_veg = dataset_creation(path, class_train = 'vegetation', batch_size = 8)\ntrain_batches_power, validation_batches_power, test_batches_power = dataset_creation(path, class_train = 'powerline', batch_size = 8)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T05:00:18.877661Z","iopub.execute_input":"2023-05-04T05:00:18.878672Z","iopub.status.idle":"2023-05-04T05:00:22.459362Z","shell.execute_reply.started":"2023-05-04T05:00:18.878621Z","shell.execute_reply":"2023-05-04T05:00:22.458254Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"#### **Metrics for power_line**","metadata":{}},{"cell_type":"code","source":"metrics_models(model = model_power, type_prediction = 'power_line', test_batches = test_batches_power)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T05:00:27.863429Z","iopub.execute_input":"2023-05-04T05:00:27.864603Z","iopub.status.idle":"2023-05-04T05:01:12.654924Z","shell.execute_reply.started":"2023-05-04T05:00:27.864565Z","shell.execute_reply":"2023-05-04T05:01:12.653316Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 567ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 47ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 3s 3s/step\nClass           F1         Jaccard   \n-----------------------------------\nbackground     : 0.02549 - 0.01302\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2751140174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'power_line'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_batches_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/3387851004.py\u001b[0m in \u001b[0;36mmetrics_models\u001b[0;34m(model, type_prediction, test_batches)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mclass_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mdstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"],"ename":"IndexError","evalue":"index 1 is out of bounds for axis 1 with size 1","output_type":"error"}]},{"cell_type":"code","source":"show_predictions(dataset = test_batches_power, model = model_power, num = 5)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T05:04:48.427493Z","iopub.execute_input":"2023-05-04T05:04:48.428480Z","iopub.status.idle":"2023-05-04T05:04:56.907635Z","shell.execute_reply.started":"2023-05-04T05:04:48.428430Z","shell.execute_reply":"2023-05-04T05:04:56.906622Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 29ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n array([[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n \n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n \n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n \n        ...,\n \n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n \n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n \n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        ...,\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>]"},"metadata":{}},{"name":"stdout","text":"1/1 [==============================] - 0s 29ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n array([[[0.42745098, 0.42352942, 0.29411766],\n         [0.48235294, 0.47843137, 0.36078432],\n         [0.44705883, 0.44705883, 0.33333334],\n         ...,\n         [0.37254903, 0.4117647 , 0.38039216],\n         [0.38431373, 0.42352942, 0.39607844],\n         [0.4       , 0.44705883, 0.4117647 ]],\n \n        [[0.48235294, 0.47843137, 0.3372549 ],\n         [0.49019608, 0.47843137, 0.3372549 ],\n         [0.49019608, 0.47843137, 0.34509805],\n         ...,\n         [0.4       , 0.43529412, 0.4       ],\n         [0.4627451 , 0.49411765, 0.46666667],\n         [0.48235294, 0.52156866, 0.5019608 ]],\n \n        [[0.4745098 , 0.47058824, 0.32941177],\n         [0.49019608, 0.4862745 , 0.34901962],\n         [0.5019608 , 0.49411765, 0.36862746],\n         ...,\n         [0.39607844, 0.42745098, 0.3882353 ],\n         [0.48235294, 0.50980395, 0.4627451 ],\n         [0.5176471 , 0.54509807, 0.5058824 ]],\n \n        ...,\n \n        [[0.5019608 , 0.5764706 , 0.654902  ],\n         [0.5294118 , 0.5882353 , 0.6745098 ],\n         [0.54509807, 0.6039216 , 0.6784314 ],\n         ...,\n         [0.5176471 , 0.5764706 , 0.60784316],\n         [0.53333336, 0.5882353 , 0.61960787],\n         [0.53333336, 0.5882353 , 0.62352943]],\n \n        [[0.5019608 , 0.56078434, 0.65882355],\n         [0.5254902 , 0.5803922 , 0.67058825],\n         [0.54509807, 0.6       , 0.68235296],\n         ...,\n         [0.5254902 , 0.5803922 , 0.6117647 ],\n         [0.5254902 , 0.5803922 , 0.6117647 ],\n         [0.52156866, 0.5803922 , 0.6117647 ]],\n \n        [[0.5176471 , 0.5764706 , 0.6745098 ],\n         [0.52156866, 0.5764706 , 0.67058825],\n         [0.5372549 , 0.59607846, 0.6784314 ],\n         ...,\n         [0.54901963, 0.5921569 , 0.6313726 ],\n         [0.50980395, 0.5529412 , 0.5882353 ],\n         [0.5137255 , 0.5686275 , 0.6       ]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        ...,\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>]"},"metadata":{}},{"name":"stdout","text":"1/1 [==============================] - 0s 30ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n array([[[0.5803922 , 0.6313726 , 0.39215687],\n         [0.5568628 , 0.61960787, 0.31764707],\n         [0.4745098 , 0.5529412 , 0.23137255],\n         ...,\n         [0.40392157, 0.43529412, 0.27058825],\n         [0.35686275, 0.37254903, 0.21960784],\n         [0.15686275, 0.16862746, 0.02745098]],\n \n        [[0.6       , 0.6392157 , 0.43137255],\n         [0.5372549 , 0.6039216 , 0.30588236],\n         [0.5019608 , 0.6       , 0.2901961 ],\n         ...,\n         [0.12941177, 0.13725491, 0.01176471],\n         [0.39215687, 0.40784314, 0.26666668],\n         [0.38039216, 0.40784314, 0.23529412]],\n \n        [[0.7058824 , 0.78431374, 0.56078434],\n         [0.62352943, 0.69411767, 0.42745098],\n         [0.54509807, 0.5921569 , 0.3647059 ],\n         ...,\n         [0.22745098, 0.23137255, 0.07843138],\n         [0.19607843, 0.20392157, 0.05098039],\n         [0.34509805, 0.37254903, 0.20784314]],\n \n        ...,\n \n        [[0.6392157 , 0.58431375, 0.45882353],\n         [0.63529414, 0.5921569 , 0.47058824],\n         [0.68235296, 0.6745098 , 0.54901963],\n         ...,\n         [0.6313726 , 0.7254902 , 0.627451  ],\n         [0.5058824 , 0.5411765 , 0.45882353],\n         [0.27450982, 0.32156864, 0.25882354]],\n \n        [[0.6509804 , 0.61960787, 0.49411765],\n         [0.6745098 , 0.6431373 , 0.50980395],\n         [0.7411765 , 0.7058824 , 0.5529412 ],\n         ...,\n         [0.43137255, 0.4862745 , 0.39215687],\n         [0.5294118 , 0.57254905, 0.49803922],\n         [0.4509804 , 0.48235294, 0.41960785]],\n \n        [[0.7411765 , 0.7176471 , 0.5764706 ],\n         [0.74509805, 0.7254902 , 0.58431375],\n         [0.7921569 , 0.76862746, 0.61960787],\n         ...,\n         [0.15686275, 0.20392157, 0.14117648],\n         [0.28235295, 0.3764706 , 0.25882354],\n         [0.41568628, 0.42745098, 0.36862746]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        ...,\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>]"},"metadata":{}},{"name":"stdout","text":"1/1 [==============================] - 0s 31ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n array([[[0.35686275, 0.39215687, 0.22352941],\n         [0.27450982, 0.32156864, 0.14901961],\n         [0.2784314 , 0.33333334, 0.11764706],\n         ...,\n         [0.89411765, 0.92941177, 0.88235295],\n         [0.827451  , 0.8235294 , 0.80784315],\n         [0.68235296, 0.6862745 , 0.6431373 ]],\n \n        [[0.38431373, 0.4       , 0.21568628],\n         [0.37254903, 0.4       , 0.21568628],\n         [0.3647059 , 0.40392157, 0.19607843],\n         ...,\n         [0.9098039 , 0.9529412 , 0.89411765],\n         [0.9019608 , 0.8784314 , 0.88235295],\n         [0.7647059 , 0.76862746, 0.7294118 ]],\n \n        [[0.25882354, 0.2509804 , 0.09019608],\n         [0.29803923, 0.29803923, 0.1254902 ],\n         [0.29803923, 0.31764707, 0.12941177],\n         ...,\n         [0.8980392 , 0.92941177, 0.89411765],\n         [0.9098039 , 0.91764706, 0.90588236],\n         [0.84313726, 0.8666667 , 0.827451  ]],\n \n        ...,\n \n        [[0.36078432, 0.38431373, 0.14901961],\n         [0.19215687, 0.22745098, 0.        ],\n         [0.18431373, 0.1882353 , 0.        ],\n         ...,\n         [0.49019608, 0.53333336, 0.20392157],\n         [0.41568628, 0.43137255, 0.10588235],\n         [0.3647059 , 0.41960785, 0.10196079]],\n \n        [[0.36862746, 0.43529412, 0.14509805],\n         [0.30980393, 0.33333334, 0.01568628],\n         [0.47058824, 0.4862745 , 0.20392157],\n         ...,\n         [0.33333334, 0.37254903, 0.03921569],\n         [0.30980393, 0.30980393, 0.        ],\n         [0.33333334, 0.37254903, 0.02352941]],\n \n        [[0.2784314 , 0.30980393, 0.02352941],\n         [0.26666668, 0.29411766, 0.05882353],\n         [0.40392157, 0.46666667, 0.18431373],\n         ...,\n         [0.3647059 , 0.4       , 0.0627451 ],\n         [0.28235295, 0.3254902 , 0.        ],\n         [0.3372549 , 0.35686275, 0.03921569]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        ...,\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>]"},"metadata":{}},{"name":"stdout","text":"1/1 [==============================] - 0s 29ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n array([[[0.5176471 , 0.5176471 , 0.49019608],\n         [0.5254902 , 0.5254902 , 0.5058824 ],\n         [0.53333336, 0.5254902 , 0.5176471 ],\n         ...,\n         [0.4117647 , 0.41960785, 0.23529412],\n         [0.4862745 , 0.50980395, 0.32156864],\n         [0.4862745 , 0.50980395, 0.32156864]],\n \n        [[0.5254902 , 0.5137255 , 0.49411765],\n         [0.53333336, 0.5176471 , 0.49803922],\n         [0.53333336, 0.52156866, 0.5019608 ],\n         ...,\n         [0.39607844, 0.40784314, 0.20784314],\n         [0.35686275, 0.39215687, 0.1764706 ],\n         [0.35686275, 0.39215687, 0.1764706 ]],\n \n        [[0.5254902 , 0.5254902 , 0.50980395],\n         [0.5254902 , 0.52156866, 0.5019608 ],\n         [0.52156866, 0.5254902 , 0.5019608 ],\n         ...,\n         [0.34117648, 0.36078432, 0.14901961],\n         [0.2509804 , 0.28235295, 0.06666667],\n         [0.2509804 , 0.28235295, 0.06666667]],\n \n        ...,\n \n        [[0.47843137, 0.49019608, 0.46666667],\n         [0.4745098 , 0.4862745 , 0.46666667],\n         [0.4745098 , 0.49411765, 0.4745098 ],\n         ...,\n         [0.78431374, 0.7882353 , 0.7607843 ],\n         [0.8039216 , 0.8039216 , 0.7764706 ],\n         [0.8039216 , 0.8039216 , 0.7764706 ]],\n \n        [[0.4862745 , 0.4862745 , 0.46666667],\n         [0.48235294, 0.4862745 , 0.46666667],\n         [0.47843137, 0.4862745 , 0.47058824],\n         ...,\n         [0.8352941 , 0.84313726, 0.8117647 ],\n         [0.89411765, 0.90588236, 0.87058824],\n         [0.89411765, 0.90588236, 0.87058824]],\n \n        [[0.4862745 , 0.4862745 , 0.46666667],\n         [0.48235294, 0.4862745 , 0.46666667],\n         [0.47843137, 0.4862745 , 0.47058824],\n         ...,\n         [0.8352941 , 0.84313726, 0.8117647 ],\n         [0.89411765, 0.90588236, 0.87058824],\n         [0.89411765, 0.90588236, 0.87058824]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        ...,\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>]"},"metadata":{}}]},{"cell_type":"markdown","source":"#### **Metrics for vegetation**","metadata":{}},{"cell_type":"code","source":"metrics_models(model = model_veg, type_prediction = 'vegetation', test_batches = test_batches_veg)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T05:06:36.525472Z","iopub.execute_input":"2023-05-04T05:06:36.526578Z","iopub.status.idle":"2023-05-04T05:07:17.156201Z","shell.execute_reply.started":"2023-05-04T05:06:36.526537Z","shell.execute_reply":"2023-05-04T05:07:17.155028Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 90ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\nClass           F1         Jaccard   \n-----------------------------------\nbackground     : 0.51211 - 0.37974\nvegetation     : 0.00078 - 0.00039\n-----------------------------------\nMean           : 0.25644 - 0.19006\n","output_type":"stream"}]},{"cell_type":"code","source":"show_predictions(dataset = test_batches_veg, model = model_veg, num = 5)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T05:09:25.394029Z","iopub.execute_input":"2023-05-04T05:09:25.395126Z","iopub.status.idle":"2023-05-04T05:09:35.676850Z","shell.execute_reply.started":"2023-05-04T05:09:25.395088Z","shell.execute_reply":"2023-05-04T05:09:35.675712Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 30ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n array([[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n \n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n \n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n \n        ...,\n \n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n \n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n \n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.],\n         ...,\n         [0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [1.],\n         [1.]],\n \n        ...,\n \n        [[1.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [0.],\n         ...,\n         [0.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>]"},"metadata":{}},{"name":"stdout","text":"1/1 [==============================] - 0s 28ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n array([[[0.42745098, 0.42352942, 0.29411766],\n         [0.48235294, 0.47843137, 0.36078432],\n         [0.44705883, 0.44705883, 0.33333334],\n         ...,\n         [0.37254903, 0.4117647 , 0.38039216],\n         [0.38431373, 0.42352942, 0.39607844],\n         [0.4       , 0.44705883, 0.4117647 ]],\n \n        [[0.48235294, 0.47843137, 0.3372549 ],\n         [0.49019608, 0.47843137, 0.3372549 ],\n         [0.49019608, 0.47843137, 0.34509805],\n         ...,\n         [0.4       , 0.43529412, 0.4       ],\n         [0.4627451 , 0.49411765, 0.46666667],\n         [0.48235294, 0.52156866, 0.5019608 ]],\n \n        [[0.4745098 , 0.47058824, 0.32941177],\n         [0.49019608, 0.4862745 , 0.34901962],\n         [0.5019608 , 0.49411765, 0.36862746],\n         ...,\n         [0.39607844, 0.42745098, 0.3882353 ],\n         [0.48235294, 0.50980395, 0.4627451 ],\n         [0.5176471 , 0.54509807, 0.5058824 ]],\n \n        ...,\n \n        [[0.5019608 , 0.5764706 , 0.654902  ],\n         [0.5294118 , 0.5882353 , 0.6745098 ],\n         [0.54509807, 0.6039216 , 0.6784314 ],\n         ...,\n         [0.5176471 , 0.5764706 , 0.60784316],\n         [0.53333336, 0.5882353 , 0.61960787],\n         [0.53333336, 0.5882353 , 0.62352943]],\n \n        [[0.5019608 , 0.56078434, 0.65882355],\n         [0.5254902 , 0.5803922 , 0.67058825],\n         [0.54509807, 0.6       , 0.68235296],\n         ...,\n         [0.5254902 , 0.5803922 , 0.6117647 ],\n         [0.5254902 , 0.5803922 , 0.6117647 ],\n         [0.52156866, 0.5803922 , 0.6117647 ]],\n \n        [[0.5176471 , 0.5764706 , 0.6745098 ],\n         [0.52156866, 0.5764706 , 0.67058825],\n         [0.5372549 , 0.59607846, 0.6784314 ],\n         ...,\n         [0.54901963, 0.5921569 , 0.6313726 ],\n         [0.50980395, 0.5529412 , 0.5882353 ],\n         [0.5137255 , 0.5686275 , 0.6       ]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [0.],\n         ...,\n         [0.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [1.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [1.]],\n \n        [[1.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [0.],\n         ...,\n         [0.],\n         [1.],\n         [1.]]], dtype=float32)>]"},"metadata":{}},{"name":"stdout","text":"1/1 [==============================] - 0s 31ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n array([[[0.5803922 , 0.6313726 , 0.39215687],\n         [0.5568628 , 0.61960787, 0.31764707],\n         [0.4745098 , 0.5529412 , 0.23137255],\n         ...,\n         [0.40392157, 0.43529412, 0.27058825],\n         [0.35686275, 0.37254903, 0.21960784],\n         [0.15686275, 0.16862746, 0.02745098]],\n \n        [[0.6       , 0.6392157 , 0.43137255],\n         [0.5372549 , 0.6039216 , 0.30588236],\n         [0.5019608 , 0.6       , 0.2901961 ],\n         ...,\n         [0.12941177, 0.13725491, 0.01176471],\n         [0.39215687, 0.40784314, 0.26666668],\n         [0.38039216, 0.40784314, 0.23529412]],\n \n        [[0.7058824 , 0.78431374, 0.56078434],\n         [0.62352943, 0.69411767, 0.42745098],\n         [0.54509807, 0.5921569 , 0.3647059 ],\n         ...,\n         [0.22745098, 0.23137255, 0.07843138],\n         [0.19607843, 0.20392157, 0.05098039],\n         [0.34509805, 0.37254903, 0.20784314]],\n \n        ...,\n \n        [[0.6392157 , 0.58431375, 0.45882353],\n         [0.63529414, 0.5921569 , 0.47058824],\n         [0.68235296, 0.6745098 , 0.54901963],\n         ...,\n         [0.6313726 , 0.7254902 , 0.627451  ],\n         [0.5058824 , 0.5411765 , 0.45882353],\n         [0.27450982, 0.32156864, 0.25882354]],\n \n        [[0.6509804 , 0.61960787, 0.49411765],\n         [0.6745098 , 0.6431373 , 0.50980395],\n         [0.7411765 , 0.7058824 , 0.5529412 ],\n         ...,\n         [0.43137255, 0.4862745 , 0.39215687],\n         [0.5294118 , 0.57254905, 0.49803922],\n         [0.4509804 , 0.48235294, 0.41960785]],\n \n        [[0.7411765 , 0.7176471 , 0.5764706 ],\n         [0.74509805, 0.7254902 , 0.58431375],\n         [0.7921569 , 0.76862746, 0.61960787],\n         ...,\n         [0.15686275, 0.20392157, 0.14117648],\n         [0.28235295, 0.3764706 , 0.25882354],\n         [0.41568628, 0.42745098, 0.36862746]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[0.],\n         [0.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [1.],\n         [1.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [1.]],\n \n        [[1.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>]"},"metadata":{}},{"name":"stdout","text":"1/1 [==============================] - 0s 33ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n array([[[0.35686275, 0.39215687, 0.22352941],\n         [0.27450982, 0.32156864, 0.14901961],\n         [0.2784314 , 0.33333334, 0.11764706],\n         ...,\n         [0.89411765, 0.92941177, 0.88235295],\n         [0.827451  , 0.8235294 , 0.80784315],\n         [0.68235296, 0.6862745 , 0.6431373 ]],\n \n        [[0.38431373, 0.4       , 0.21568628],\n         [0.37254903, 0.4       , 0.21568628],\n         [0.3647059 , 0.40392157, 0.19607843],\n         ...,\n         [0.9098039 , 0.9529412 , 0.89411765],\n         [0.9019608 , 0.8784314 , 0.88235295],\n         [0.7647059 , 0.76862746, 0.7294118 ]],\n \n        [[0.25882354, 0.2509804 , 0.09019608],\n         [0.29803923, 0.29803923, 0.1254902 ],\n         [0.29803923, 0.31764707, 0.12941177],\n         ...,\n         [0.8980392 , 0.92941177, 0.89411765],\n         [0.9098039 , 0.91764706, 0.90588236],\n         [0.84313726, 0.8666667 , 0.827451  ]],\n \n        ...,\n \n        [[0.36078432, 0.38431373, 0.14901961],\n         [0.19215687, 0.22745098, 0.        ],\n         [0.18431373, 0.1882353 , 0.        ],\n         ...,\n         [0.49019608, 0.53333336, 0.20392157],\n         [0.41568628, 0.43137255, 0.10588235],\n         [0.3647059 , 0.41960785, 0.10196079]],\n \n        [[0.36862746, 0.43529412, 0.14509805],\n         [0.30980393, 0.33333334, 0.01568628],\n         [0.47058824, 0.4862745 , 0.20392157],\n         ...,\n         [0.33333334, 0.37254903, 0.03921569],\n         [0.30980393, 0.30980393, 0.        ],\n         [0.33333334, 0.37254903, 0.02352941]],\n \n        [[0.2784314 , 0.30980393, 0.02352941],\n         [0.26666668, 0.29411766, 0.05882353],\n         [0.40392157, 0.46666667, 0.18431373],\n         ...,\n         [0.3647059 , 0.4       , 0.0627451 ],\n         [0.28235295, 0.3254902 , 0.        ],\n         [0.3372549 , 0.35686275, 0.03921569]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        ...,\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [1.],\n         ...,\n         [0.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [1.]],\n \n        ...,\n \n        [[1.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [0.],\n         ...,\n         [0.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>]"},"metadata":{}},{"name":"stdout","text":"1/1 [==============================] - 0s 32ms/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[<tf.Tensor: shape=(256, 256, 3), dtype=float32, numpy=\n array([[[0.5176471 , 0.5176471 , 0.49019608],\n         [0.5254902 , 0.5254902 , 0.5058824 ],\n         [0.53333336, 0.5254902 , 0.5176471 ],\n         ...,\n         [0.4117647 , 0.41960785, 0.23529412],\n         [0.4862745 , 0.50980395, 0.32156864],\n         [0.4862745 , 0.50980395, 0.32156864]],\n \n        [[0.5254902 , 0.5137255 , 0.49411765],\n         [0.53333336, 0.5176471 , 0.49803922],\n         [0.53333336, 0.52156866, 0.5019608 ],\n         ...,\n         [0.39607844, 0.40784314, 0.20784314],\n         [0.35686275, 0.39215687, 0.1764706 ],\n         [0.35686275, 0.39215687, 0.1764706 ]],\n \n        [[0.5254902 , 0.5254902 , 0.50980395],\n         [0.5254902 , 0.52156866, 0.5019608 ],\n         [0.52156866, 0.5254902 , 0.5019608 ],\n         ...,\n         [0.34117648, 0.36078432, 0.14901961],\n         [0.2509804 , 0.28235295, 0.06666667],\n         [0.2509804 , 0.28235295, 0.06666667]],\n \n        ...,\n \n        [[0.47843137, 0.49019608, 0.46666667],\n         [0.4745098 , 0.4862745 , 0.46666667],\n         [0.4745098 , 0.49411765, 0.4745098 ],\n         ...,\n         [0.78431374, 0.7882353 , 0.7607843 ],\n         [0.8039216 , 0.8039216 , 0.7764706 ],\n         [0.8039216 , 0.8039216 , 0.7764706 ]],\n \n        [[0.4862745 , 0.4862745 , 0.46666667],\n         [0.48235294, 0.4862745 , 0.46666667],\n         [0.47843137, 0.4862745 , 0.47058824],\n         ...,\n         [0.8352941 , 0.84313726, 0.8117647 ],\n         [0.89411765, 0.90588236, 0.87058824],\n         [0.89411765, 0.90588236, 0.87058824]],\n \n        [[0.4862745 , 0.4862745 , 0.46666667],\n         [0.48235294, 0.4862745 , 0.46666667],\n         [0.47843137, 0.4862745 , 0.47058824],\n         ...,\n         [0.8352941 , 0.84313726, 0.8117647 ],\n         [0.89411765, 0.90588236, 0.87058824],\n         [0.89411765, 0.90588236, 0.87058824]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[0.],\n         [0.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]],\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [0.]]], dtype=float32)>,\n <tf.Tensor: shape=(256, 256, 1), dtype=float32, numpy=\n array([[[1.],\n         [1.],\n         [1.],\n         ...,\n         [1.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [0.],\n         ...,\n         [0.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [1.]],\n \n        ...,\n \n        [[0.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [0.],\n         [1.]],\n \n        [[1.],\n         [0.],\n         [0.],\n         ...,\n         [0.],\n         [1.],\n         [1.]],\n \n        [[1.],\n         [1.],\n         [0.],\n         ...,\n         [1.],\n         [1.],\n         [1.]]], dtype=float32)>]"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Union of both models**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport cv2\nimport numpy as np\n\n# Cargar los pesos pre-entrenados de las dos redes neuronales\n#model_vegetacion = tf.keras.models.load_model(\"modelo_vegetacion.h5\")\n#model_linea_electrica = tf.keras.models.load_model(\"modelo_linea_electrica.h5\")\n\nfor image, mask in test_batches_veg.skip(2).take(1):\n\n    # Realizar la segmentación semántica para obtener las dos máscaras\n    mask_vegetacion = model_veg.predict(image)\n    mask_vegetacion = create_mask(mask_vegetacion, threshold=0.25)\n    mask_linea_electrica = model_power.predict(image)\n    mask_linea_electrica = create_mask(mask_linea_electrica, threshold=0.25)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T05:04:01.999219Z","iopub.execute_input":"2023-05-04T05:04:02.000500Z","iopub.status.idle":"2023-05-04T05:04:10.589125Z","shell.execute_reply.started":"2023-05-04T05:04:02.000458Z","shell.execute_reply":"2023-05-04T05:04:10.587991Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 548ms/step\n1/1 [==============================] - 0s 29ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_mask = np.zeros_like(image.numpy()[0])\ncombined_mask[np.squeeze(mask_vegetacion == 1), :] = (0, 255, 0) # verde para mask_vegetacion\ncombined_mask[np.squeeze(mask_linea_electrica == 1), :] = (110, 110, 110) # gris para mask_linea_electrica\nfinal_image = cv2.addWeighted(image.numpy()[0], 0.5, combined_mask, 0.5, 0)","metadata":{"execution":{"iopub.status.busy":"2023-05-04T05:04:13.744223Z","iopub.execute_input":"2023-05-04T05:04:13.744960Z","iopub.status.idle":"2023-05-04T05:04:13.772189Z","shell.execute_reply.started":"2023-05-04T05:04:13.744923Z","shell.execute_reply":"2023-05-04T05:04:13.771159Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"final_image = np.array(final_image, dtype=np.uint8)\n\n# Visualizar la imagen resultante\nplt.imshow(final_image)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-04T05:04:15.849113Z","iopub.execute_input":"2023-05-04T05:04:15.850109Z","iopub.status.idle":"2023-05-04T05:04:16.094820Z","shell.execute_reply.started":"2023-05-04T05:04:15.850069Z","shell.execute_reply":"2023-05-04T05:04:16.093852Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# Crear una máscara para los píxeles que son verdes o grises\nmask_combined = np.logical_and(mask_vegetacion, mask_linea_electrica)\nmask_combined = np.squeeze(mask_combined)\n\n# Convertir la imagen a un array de numpy y cambiar el tipo de dato a uint8\nfinal_image = np.array(final_image, dtype=np.uint8)\n\n# Aplicar la máscara a la imagen original\nfinal_image[mask_combined, :] = [255, 0, 0]  # Rojo: [R, G, B]\n\n# Visualizar la imagen resultante\nplt.imshow(final_image)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-04T05:04:38.448715Z","iopub.execute_input":"2023-05-04T05:04:38.449353Z","iopub.status.idle":"2023-05-04T05:04:38.665314Z","shell.execute_reply.started":"2023-05-04T05:04:38.449312Z","shell.execute_reply":"2023-05-04T05:04:38.664241Z"},"trusted":true},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Initial Approach**","metadata":{}},{"cell_type":"markdown","source":"## **Creating new mask folders**","metadata":{}},{"cell_type":"markdown","source":"### **MASK_NEW_VEGETATION**","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\n\n# Definir la ruta de la carpeta con las máscaras\nfolder_path = \"/kaggle/input/tesellated-with-geometric-augmentation/TESELLATED_WITH_GEOMETRIC_AUGMENTATION/MASK\"\n\n# Crear una nueva carpeta para las imágenes binarias\nbinary_folder_path = \"/kaggle/working/MASK_NEW_VEGETATION\"\n\nif not os.path.exists(binary_folder_path):\n    os.mkdir(binary_folder_path)\n\n# Leer cada archivo en la carpeta y procesar\nfor filename in os.listdir(folder_path):\n  # Cargar la imagen y convertir a escala de grises\n  img = cv2.imread(os.path.join(folder_path, filename))\n  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n  # Aplicar la umbralización\n  _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n\n  # Convertir los píxeles verde (vegetación) en blanco y los píxeles grises (líneas eléctricas) en negro\n  thresh[(img[:, :, 1] == 255) & (img[:, :, 0] == 0)] = 255\n  thresh[(img[:, :, 0] == img[:, :, 1]) & (img[:, :, 0] == img[:, :, 2]) & (img[:, :, 0] != 0)] = 0\n\n  # Guardar la imagen binaria en la nueva carpeta\n  cv2.imwrite(os.path.join(binary_folder_path, filename), thresh)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:44:14.495155Z","iopub.execute_input":"2023-04-27T02:44:14.496007Z","iopub.status.idle":"2023-04-27T02:45:00.176482Z","shell.execute_reply.started":"2023-04-27T02:44:14.495960Z","shell.execute_reply":"2023-04-27T02:45:00.175212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **MASK_NEW_POWERLINE**","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\n\n# Definir la ruta de la carpeta con las máscaras\nfolder_path = \"/kaggle/input/tesellated-with-geometric-augmentation/TESELLATED_WITH_GEOMETRIC_AUGMENTATION/MASK\"\n\n# Crear una nueva carpeta para las imágenes binarias\nbinary_folder_path = \"/kaggle/working/MASK_NEW_POWERLINE\"\n\nif not os.path.exists(binary_folder_path):\n    os.mkdir(binary_folder_path)\n\n# Leer cada archivo en la carpeta y procesar\nfor filename in os.listdir(folder_path):\n  # Cargar la imagen y convertir a escala de grises\n  img = cv2.imread(os.path.join(folder_path, filename))\n  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n  # Aplicar la umbralización\n  _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n\n  # Convertir los píxeles (110, 110, 110) en blanco y todo lo demás en negro\n  thresh[(img[:, :, 0] == 110) & (img[:, :, 1] == 110) & (img[:, :, 2] == 110)] = 255\n  thresh[(img[:, :, 0] != 110) | (img[:, :, 1] != 110) | (img[:, :, 2] != 110)] = 0\n\n  # Guardar la imagen binaria en la nueva carpeta\n  cv2.imwrite(os.path.join(binary_folder_path, filename), thresh)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:00.180260Z","iopub.execute_input":"2023-04-27T02:45:00.180596Z","iopub.status.idle":"2023-04-27T02:45:13.779117Z","shell.execute_reply.started":"2023-04-27T02:45:00.180564Z","shell.execute_reply":"2023-04-27T02:45:13.778096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Creating loss functions**","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n\nclass LossFunctions:\n    @staticmethod\n    def tversky(y_true, y_pred):\n        y_true_pos = K.flatten(y_true)\n        y_pred_pos = K.flatten(y_pred)\n        true_pos = K.sum(y_true_pos * y_pred_pos)\n        false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n        false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n        alpha = 0.7\n        smooth = 1e-5\n        return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\n    @staticmethod\n    def tversky_loss(y_true, y_pred):\n        return 1 - LossFunctions.tversky(y_true,y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:13.782405Z","iopub.execute_input":"2023-04-27T02:45:13.783205Z","iopub.status.idle":"2023-04-27T02:45:13.791319Z","shell.execute_reply.started":"2023-04-27T02:45:13.783163Z","shell.execute_reply":"2023-04-27T02:45:13.790377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n\ndef categorical_focal_loss(gamma=2.0, alpha_val=None):\n    def focal_loss(y_true, y_pred):\n        \"\"\"\n        Categorical focal loss function.\n        :param y_true: one-hot encoded true labels\n        :param y_pred: predicted label probabilities\n        :param gamma:\n        :param alpha:\n        :return:\n        \"\"\"\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n\n        cross_entropy = -y_true * K.log(y_pred)\n\n        if alpha_val is not None:\n            alpha = K.constant(alpha_val)\n            class_weights = K.sum(alpha * y_true, axis=-1)\n            class_weights = K.expand_dims(class_weights, axis=-1)\n            focal_weights = class_weights * K.pow(1 - y_pred, gamma)\n            loss = focal_weights * cross_entropy\n        else:\n            focal_weights = K.pow(1 - y_pred, gamma)\n            loss = focal_weights * cross_entropy\n\n        return K.mean(loss)\n\n    return focal_loss","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:13.792906Z","iopub.execute_input":"2023-04-27T02:45:13.793653Z","iopub.status.idle":"2023-04-27T02:45:13.806836Z","shell.execute_reply.started":"2023-04-27T02:45:13.793617Z","shell.execute_reply":"2023-04-27T02:45:13.805897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n\ndef binary_focal_loss(gamma=2.0, alpha=0.25):\n    def focal_loss(y_true, y_pred):\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n        pos_weight = alpha * y_true + (1 - alpha) * (1 - y_true)\n        focal_weights = pos_weight * K.pow(1 - y_pred, gamma)\n        loss = focal_weights * K.binary_crossentropy(y_true, y_pred)\n        return K.mean(loss)\n    return focal_loss","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:13.808389Z","iopub.execute_input":"2023-04-27T02:45:13.809242Z","iopub.status.idle":"2023-04-27T02:45:13.825957Z","shell.execute_reply.started":"2023-04-27T02:45:13.809205Z","shell.execute_reply":"2023-04-27T02:45:13.825083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n\ndef weighted_binary_crossentropy(pos_weight=0.25):\n    def weighted_bce(y_true, y_pred):\n        epsilon = K.epsilon()\n        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n        loss = pos_weight * y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred)\n        return -K.mean(loss)\n    return weighted_bce","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:13.827408Z","iopub.execute_input":"2023-04-27T02:45:13.828179Z","iopub.status.idle":"2023-04-27T02:45:13.840400Z","shell.execute_reply.started":"2023-04-27T02:45:13.828140Z","shell.execute_reply":"2023-04-27T02:45:13.839262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Building dataset for only veg**","metadata":{}},{"cell_type":"code","source":"# ruta a las carpetas de imágenes y máscaras\ndata_dir = pathlib.Path('/kaggle/input/tesellated-with-geometric-augmentation/TESELLATED_WITH_GEOMETRIC_AUGMENTATION/')\nimages_dir = data_dir / 'RGB'\ndata_mask_dir = pathlib.Path('/kaggle/working/')\nmasks_dir = data_mask_dir / \"MASK_NEW_VEGETATION\"\n\n# obtener una lista de rutas de archivo para imágenes y máscaras\nimage_paths = sorted([str(path) for path in images_dir.glob('*.png')])\nmask_paths = sorted([str(path) for path in masks_dir.glob('*.png')])\n\n# crear un dataset a partir de las rutas de archivo\ndataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n\n# definir una función para cargar y preprocesar las imágenes y máscaras\ndef load_and_preprocess_image(image_path, mask_path):\n    # cargar la imagen y máscara\n    image = tf.io.decode_png(tf.io.read_file(image_path))\n    mask = tf.io.decode_png(tf.io.read_file(mask_path), channels=1)\n    \n    # normalizar la imagen y máscara\n    image = tf.cast(image, tf.float32) / 255.0\n    mask = tf.cast(mask, tf.float32) / 255.0\n    \n    return image, mask\n\n# aplicar la función de carga y preprocesamiento a cada par de rutas de archivo en el dataset\ndataset = dataset.map(load_and_preprocess_image)\n\n# dividir el dataset en conjuntos de entrenamiento, validación y prueba\ntotal_samples = len(image_paths)\ntrain_size = int(0.8 * total_samples)\nval_size = int(0.1 * total_samples)\ntest_size = total_samples - train_size - val_size\n\ntrain_dataset = dataset.take(train_size)\nval_dataset = dataset.skip(train_size).take(val_size)\ntest_dataset = dataset.skip(train_size + val_size).take(test_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:13.841974Z","iopub.execute_input":"2023-04-27T02:45:13.842578Z","iopub.status.idle":"2023-04-27T02:45:19.462118Z","shell.execute_reply.started":"2023-04-27T02:45:13.842543Z","shell.execute_reply":"2023-04-27T02:45:19.461093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nBUFFER_SIZE = 100","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:19.463698Z","iopub.execute_input":"2023-04-27T02:45:19.464104Z","iopub.status.idle":"2023-04-27T02:45:19.470205Z","shell.execute_reply.started":"2023-04-27T02:45:19.464050Z","shell.execute_reply":"2023-04-27T02:45:19.468869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_batches = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_batches = train_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nvalidation_batches = test_dataset.batch(BATCH_SIZE)\ntest_batches = test_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:19.478122Z","iopub.execute_input":"2023-04-27T02:45:19.478813Z","iopub.status.idle":"2023-04-27T02:45:19.510870Z","shell.execute_reply.started":"2023-04-27T02:45:19.478765Z","shell.execute_reply":"2023-04-27T02:45:19.509769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_size, val_size,test_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:19.512181Z","iopub.execute_input":"2023-04-27T02:45:19.512534Z","iopub.status.idle":"2023-04-27T02:45:19.519252Z","shell.execute_reply.started":"2023-04-27T02:45:19.512495Z","shell.execute_reply":"2023-04-27T02:45:19.518020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display(display_list):\n  plt.figure(figsize=(15, 15))\n\n  title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(title[i])\n    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n    plt.axis(\"off\")\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:19.521065Z","iopub.execute_input":"2023-04-27T02:45:19.521489Z","iopub.status.idle":"2023-04-27T02:45:19.533334Z","shell.execute_reply.started":"2023-04-27T02:45:19.521452Z","shell.execute_reply":"2023-04-27T02:45:19.532396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_batch = next(iter(train_batches))\nrandom_index = np.random.choice(sample_batch[0].shape[0])\nsample_image, sample_mask = sample_batch[0][random_index], sample_batch[1][random_index]\ndisplay([sample_image, sample_mask])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:19.535001Z","iopub.execute_input":"2023-04-27T02:45:19.535448Z","iopub.status.idle":"2023-04-27T02:45:21.068548Z","shell.execute_reply.started":"2023-04-27T02:45:19.535410Z","shell.execute_reply":"2023-04-27T02:45:21.067588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Architecture U-Net**","metadata":{}},{"cell_type":"code","source":"def double_conv_block(x, n_filters):\n\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    # dropout\n    x = layers.Dropout(0.4)(x)\n\n    return x\n\ndef downsample_block(x, n_filters):\n    f = double_conv_block(x, n_filters)\n    p = layers.MaxPool2D(2)(f)\n    p = layers.Dropout(0.4)(p)\n\n    return f, p\n\ndef upsample_block(x, conv_features, n_filters):\n    # upsample\n    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n    # concatenate \n    x = layers.concatenate([x, conv_features])\n    # dropout\n    x = layers.Dropout(0.4)(x)\n    # Conv2D twice with ReLU activation\n    x = double_conv_block(x, n_filters)\n\n    return x\n    \ndef build_unet_model():\n\n    # inputs\n    inputs = layers.Input(shape=(256,256,3))\n\n    # encoder: contracting path - downsample\n    # 1 - downsample\n    f1, p1 = downsample_block(inputs, 64)\n    # 2 - downsample\n    f2, p2 = downsample_block(p1, 128)\n    # 3 - downsample\n    f3, p3 = downsample_block(p2, 256)\n    # 4 - downsample\n    f4, p4 = downsample_block(p3, 512)\n\n    # 5 - bottleneck\n    bottleneck = double_conv_block(p4, 1024)\n    bottleneck = layers.Dropout(0.5)(bottleneck)\n\n    # decoder: expanding path - upsample\n    # 6 - upsample\n    u6 = upsample_block(bottleneck, f4, 512)\n    # 7 - upsample\n    u7 = upsample_block(u6, f3, 256)\n    # 8 - upsample\n    u8 = upsample_block(u7, f2, 128)\n    # 9 - upsample\n    u9 = upsample_block(u8, f1, 64)\n\n    # outputs\n    outputs = layers.Conv2D(1, 1, padding=\"same\", activation = \"sigmoid\")(u9)\n\n    # unet model with Keras Functional API\n    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n\n    return unet_model","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:21.069512Z","iopub.execute_input":"2023-04-27T02:45:21.069838Z","iopub.status.idle":"2023-04-27T02:45:21.252919Z","shell.execute_reply.started":"2023-04-27T02:45:21.069803Z","shell.execute_reply":"2023-04-27T02:45:21.251724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom tensorflow.keras.layers import Conv2D, Input, MaxPooling2D, Dropout, concatenate, UpSampling2D\nfrom tensorflow.keras.models import load_model, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\nfrom tensorflow.keras import backend as K\n\n  \nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.callbacks import *\n\nimport random \n\ndef VGGUnet(image_size, vgg_weight_path=None):\n    inputs = Input((image_size, image_size, 3))\n    # Block 1\n    x = Conv2D(64, (3, 3), padding='same', name='block1_conv1')(inputs)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(64, (3, 3), padding='same', name='block1_conv2')(x)\n    x = BatchNormalization()(x)\n    block_1_out = Activation('relu')(x)\n\n    x = MaxPooling2D()(block_1_out)\n\n    # Block 2\n    x = Conv2D(128, (3, 3), padding='same', name='block2_conv1')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(128, (3, 3), padding='same', name='block2_conv2')(x)\n    x = BatchNormalization()(x)\n    block_2_out = Activation('relu')(x)\n\n    x = MaxPooling2D()(block_2_out)\n\n    # Block 3\n    x = Conv2D(256, (3, 3), padding='same', name='block3_conv1')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(256, (3, 3), padding='same', name='block3_conv2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(256, (3, 3), padding='same', name='block3_conv3')(x)\n    x = BatchNormalization()(x)\n    block_3_out = Activation('relu')(x)\n\n    x = MaxPooling2D()(block_3_out)\n\n    # Block 4\n    x = Conv2D(512, (3, 3), padding='same', name='block4_conv1')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(512, (3, 3), padding='same', name='block4_conv2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(512, (3, 3), padding='same', name='block4_conv3')(x)\n    x = BatchNormalization()(x)\n    block_4_out = Activation('relu')(x)\n\n    x = MaxPooling2D()(block_4_out)\n\n    # Block 5\n    x = Conv2D(512, (3, 3), padding='same', name='block5_conv1')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(512, (3, 3), padding='same', name='block5_conv2')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(512, (3, 3), padding='same', name='block5_conv3')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    for_pretrained_weight = MaxPooling2D()(x)\n\n    # Load pretrained weights.\n    if vgg_weight_path is not None:\n        vgg16 = Model(inputs, for_pretrained_weight)\n        vgg16.load_weights(vgg_weight_path, by_name=True)\n\n    # UP 1\n    x = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = concatenate([x, block_4_out])\n    x = Conv2D(512, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(512, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # UP 2\n    x = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = concatenate([x, block_3_out])\n    x = Conv2D(256, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(256, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # UP 3\n    x = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = concatenate([x, block_2_out])\n    x = Conv2D(128, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(128, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    # UP 4\n    x = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = concatenate([x, block_1_out])\n    x = Conv2D(64, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(64, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(1, (3, 3), padding='same')(x)\n    x = BatchNormalization()(x)\n\n    outputs = Activation('sigmoid')(x)\n\n    model = Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:21.255086Z","iopub.execute_input":"2023-04-27T02:45:21.255532Z","iopub.status.idle":"2023-04-27T02:45:21.288281Z","shell.execute_reply.started":"2023-04-27T02:45:21.255494Z","shell.execute_reply":"2023-04-27T02:45:21.287315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\nK.clear_session()\n\ntversky_loss_fn = LossFunctions.tversky_loss\nmodel_veg = VGGUnet(image_size = 256)\nmetrics = [\"accuracy\", \n           tf.keras.metrics.AUC(), \n           tf.keras.metrics.SensitivityAtSpecificity(0.5), \n           tf.keras.metrics.SpecificityAtSensitivity(0.5)]\nmodel_veg.compile(optimizer=Adam(), loss=tversky_loss_fn, metrics=metrics)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:21.290055Z","iopub.execute_input":"2023-04-27T02:45:21.290503Z","iopub.status.idle":"2023-04-27T02:45:22.017507Z","shell.execute_reply.started":"2023-04-27T02:45:21.290459Z","shell.execute_reply":"2023-04-27T02:45:22.016510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet_model = build_unet_model()\n#unet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:22.018889Z","iopub.execute_input":"2023-04-27T02:45:22.019272Z","iopub.status.idle":"2023-04-27T02:45:22.710884Z","shell.execute_reply.started":"2023-04-27T02:45:22.019229Z","shell.execute_reply":"2023-04-27T02:45:22.709728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tf.keras.utils.plot_model(unet_model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:22.712881Z","iopub.execute_input":"2023-04-27T02:45:22.713507Z","iopub.status.idle":"2023-04-27T02:45:22.718161Z","shell.execute_reply.started":"2023-04-27T02:45:22.713468Z","shell.execute_reply":"2023-04-27T02:45:22.717189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%load_ext tensorboard","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:22.719486Z","iopub.execute_input":"2023-04-27T02:45:22.720491Z","iopub.status.idle":"2023-04-27T02:45:22.728862Z","shell.execute_reply.started":"2023-04-27T02:45:22.720448Z","shell.execute_reply":"2023-04-27T02:45:22.727982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet_model.compile(optimizer=tf.keras.optimizers.Adam(),\n                   loss=\"binary_crossentropy\",\n                   metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:22.730259Z","iopub.execute_input":"2023-04-27T02:45:22.730743Z","iopub.status.idle":"2023-04-27T02:45:22.754001Z","shell.execute_reply.started":"2023-04-27T02:45:22.730700Z","shell.execute_reply":"2023-04-27T02:45:22.753059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 10\n#BATCH_SIZE = 16\n\n\nSTEPS_PER_EPOCH = total_samples // BATCH_SIZE\n\nVAL_SUBSPLITS = 5\nVALIDATION_STEPS = test_size // BATCH_SIZE // VAL_SUBSPLITS\n\ntensorboard = TensorBoard(log_dir='logs/semantic_segmentation')\nmodel_history = model_veg.fit(train_batches,\n                               epochs=NUM_EPOCHS,\n                               steps_per_epoch=STEPS_PER_EPOCH,\n                               validation_steps=VALIDATION_STEPS,\n                               validation_data=validation_batches,\n                               callbacks=[tensorboard])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T02:45:22.755554Z","iopub.execute_input":"2023-04-27T02:45:22.755975Z","iopub.status.idle":"2023-04-27T03:21:11.121391Z","shell.execute_reply.started":"2023-04-27T02:45:22.755935Z","shell.execute_reply":"2023-04-27T03:21:11.120346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%tensorboard --logdir logs","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:21:11.123093Z","iopub.execute_input":"2023-04-27T03:21:11.123556Z","iopub.status.idle":"2023-04-27T03:21:11.128870Z","shell.execute_reply.started":"2023-04-27T03:21:11.123515Z","shell.execute_reply":"2023-04-27T03:21:11.127859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Visualization of predicion**","metadata":{}},{"cell_type":"code","source":"def create_mask(pred_mask, threshold=0.3):\n  pred_mask = tf.cast(pred_mask, dtype=tf.float32)  # convertir a float\n  pred_mask = tf.where(pred_mask > threshold, 1.0, 0.0)  # usar float para el threshold\n  return pred_mask[0]\n\ndef show_predictions(dataset=None, num=1):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model_veg.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model_veg.predict(sample_image[tf.newaxis, ...]))])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:21:11.130382Z","iopub.execute_input":"2023-04-27T03:21:11.131006Z","iopub.status.idle":"2023-04-27T03:21:11.151974Z","shell.execute_reply.started":"2023-04-27T03:21:11.130969Z","shell.execute_reply":"2023-04-27T03:21:11.151089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_predictions(test_batches.skip(5), 5)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:21:11.153781Z","iopub.execute_input":"2023-04-27T03:21:11.154175Z","iopub.status.idle":"2023-04-27T03:21:31.647336Z","shell.execute_reply.started":"2023-04-27T03:21:11.154136Z","shell.execute_reply":"2023-04-27T03:21:31.646100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Metricas**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score, jaccard_score\n\n\"\"\" Prediction & Evaluation \"\"\"\nSCORE = []\nthreshold = 0.3\nnum_classes = 2\nclasses = [\n        \"background\", \"vegetation\"\n    ]\n\nfor image, mask in test_batches:\n    pred_mask = model_veg.predict(image)\n    pred_mask = tf.cast(pred_mask, dtype=tf.float32)  # convertir a float\n    pred = tf.where(pred_mask > threshold, 1.0, 0.0)\n\n    labels = list(np.unique(pred))\n    flat_mask = tf.reshape(mask,[-1])\n    flat_pred = tf.reshape(pred, [-1])\n    \n    mask_arr = flat_mask.numpy().flatten()\n    pred_arr = flat_pred.numpy().flatten()\n\n\n    \"\"\" Calculating the metrics values \"\"\"\n    f1_value = f1_score(mask_arr, pred_arr, labels=labels, average=None, zero_division=0)\n    jac_value = jaccard_score(mask_arr, pred_arr, labels=labels, average=None, zero_division=0)\n\n    SCORE.append([f1_value, jac_value])\n\nscore = np.array(SCORE)\nscore = np.mean(score, axis=0)\n\nl = [\"Class\", \"F1\", \"Jaccard\"]\nprint(f\"{l[0]:15s} {l[1]:10s} {l[2]:10s}\")\nprint(\"-\"*35)\n\nfor i in range(num_classes):\n    class_name = classes[i]\n    f1 = score[0, i]\n    jac = score[1, i]\n    dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\n    print(dstr)\n\nprint(\"-\"*35)\nclass_mean = np.mean(score, axis=-1)\nclass_name = \"Mean\"\n\nf1 = class_mean[0]\njac = class_mean[1]\n\ndstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\nprint(dstr)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:21:31.648936Z","iopub.execute_input":"2023-04-27T03:21:31.649242Z","iopub.status.idle":"2023-04-27T03:22:11.505271Z","shell.execute_reply.started":"2023-04-27T03:21:31.649213Z","shell.execute_reply":"2023-04-27T03:22:11.504182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Building dataset for only powerline**","metadata":{}},{"cell_type":"code","source":"# ruta a las carpetas de imágenes y máscaras\ndata_dir = pathlib.Path('/kaggle/input/tesellated-with-geometric-augmentation/TESELLATED_WITH_GEOMETRIC_AUGMENTATION/')\nimages_dir = data_dir / 'RGB'\ndata_mask_dir = pathlib.Path('/kaggle/working/')\nmasks_dir = data_mask_dir / \"MASK_NEW_POWERLINE\"\n\n# obtener una lista de rutas de archivo para imágenes y máscaras\nimage_paths = sorted([str(path) for path in images_dir.glob('*.png')])\nmask_paths = sorted([str(path) for path in masks_dir.glob('*.png')])\n\n# crear un dataset a partir de las rutas de archivo\ndataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n\n# definir una función para cargar y preprocesar las imágenes y máscaras\ndef load_and_preprocess_image(image_path, mask_path):\n    # cargar la imagen y máscara\n    image = tf.io.decode_png(tf.io.read_file(image_path))\n    mask = tf.io.decode_png(tf.io.read_file(mask_path), channels=1)\n    \n    # normalizar la imagen y máscara\n    image = tf.cast(image, tf.float32) / 255.0\n    mask = tf.cast(mask, tf.float32) / 255.0\n    \n    return image, mask\n\n# aplicar la función de carga y preprocesamiento a cada par de rutas de archivo en el dataset\ndataset = dataset.map(load_and_preprocess_image)\n\n# dividir el dataset en conjuntos de entrenamiento, validación y prueba\ntotal_samples = len(image_paths)\ntrain_size = int(0.8 * total_samples)\nval_size = int(0.1 * total_samples)\ntest_size = total_samples - train_size - val_size\n\ntrain_dataset = dataset.take(train_size)\nval_dataset = dataset.skip(train_size).take(val_size)\ntest_dataset = dataset.skip(train_size + val_size).take(test_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:11.506639Z","iopub.execute_input":"2023-04-27T03:22:11.506985Z","iopub.status.idle":"2023-04-27T03:22:11.597382Z","shell.execute_reply.started":"2023-04-27T03:22:11.506955Z","shell.execute_reply":"2023-04-27T03:22:11.596397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nBUFFER_SIZE = 100","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:11.598707Z","iopub.execute_input":"2023-04-27T03:22:11.599781Z","iopub.status.idle":"2023-04-27T03:22:11.604748Z","shell.execute_reply.started":"2023-04-27T03:22:11.599734Z","shell.execute_reply":"2023-04-27T03:22:11.603541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_batches = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_batches = train_batches.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\nvalidation_batches = test_dataset.batch(BATCH_SIZE)\ntest_batches = test_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:11.612420Z","iopub.execute_input":"2023-04-27T03:22:11.612690Z","iopub.status.idle":"2023-04-27T03:22:11.701572Z","shell.execute_reply.started":"2023-04-27T03:22:11.612663Z","shell.execute_reply":"2023-04-27T03:22:11.700426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_size, val_size,test_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:11.703125Z","iopub.execute_input":"2023-04-27T03:22:11.703790Z","iopub.status.idle":"2023-04-27T03:22:11.711605Z","shell.execute_reply.started":"2023-04-27T03:22:11.703749Z","shell.execute_reply":"2023-04-27T03:22:11.710077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display(display_list):\n  plt.figure(figsize=(15, 15))\n\n  title = [\"Input Image\", \"True Mask\", \"Predicted Mask\"]\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(title[i])\n    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n    plt.axis(\"off\")\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:11.713288Z","iopub.execute_input":"2023-04-27T03:22:11.714639Z","iopub.status.idle":"2023-04-27T03:22:11.722269Z","shell.execute_reply.started":"2023-04-27T03:22:11.714602Z","shell.execute_reply":"2023-04-27T03:22:11.721353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_batch = next(iter(train_batches))\nrandom_index = np.random.choice(sample_batch[0].shape[0])\nsample_image, sample_mask = sample_batch[0][random_index], sample_batch[1][random_index]\ndisplay([sample_image, sample_mask])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:11.723846Z","iopub.execute_input":"2023-04-27T03:22:11.724458Z","iopub.status.idle":"2023-04-27T03:22:12.394506Z","shell.execute_reply.started":"2023-04-27T03:22:11.724422Z","shell.execute_reply":"2023-04-27T03:22:12.393229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Architecture U-Net**","metadata":{}},{"cell_type":"code","source":"def double_conv_block(x, n_filters):\n\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    # Conv2D then ReLU activation\n    x = layers.Conv2D(n_filters, 3, padding = \"same\", kernel_initializer = \"he_normal\")(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    # dropout\n    x = layers.Dropout(0.4)(x)\n\n    return x\n\ndef downsample_block(x, n_filters):\n    f = double_conv_block(x, n_filters)\n    p = layers.MaxPool2D(2)(f)\n    p = layers.Dropout(0.4)(p)\n\n    return f, p\n\ndef upsample_block(x, conv_features, n_filters):\n    # upsample\n    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n    # concatenate \n    x = layers.concatenate([x, conv_features])\n    # dropout\n    x = layers.Dropout(0.4)(x)\n    # Conv2D twice with ReLU activation\n    x = double_conv_block(x, n_filters)\n\n    return x\n    \ndef build_unet_model():\n\n    # inputs\n    inputs = layers.Input(shape=(256,256,3))\n\n    # encoder: contracting path - downsample\n    # 1 - downsample\n    f1, p1 = downsample_block(inputs, 64)\n    # 2 - downsample\n    f2, p2 = downsample_block(p1, 128)\n    # 3 - downsample\n    f3, p3 = downsample_block(p2, 256)\n    # 4 - downsample\n    f4, p4 = downsample_block(p3, 512)\n\n    # 5 - bottleneck\n    bottleneck = double_conv_block(p4, 1024)\n    bottleneck = layers.Dropout(0.5)(bottleneck)\n\n    # decoder: expanding path - upsample\n    # 6 - upsample\n    u6 = upsample_block(bottleneck, f4, 512)\n    # 7 - upsample\n    u7 = upsample_block(u6, f3, 256)\n    # 8 - upsample\n    u8 = upsample_block(u7, f2, 128)\n    # 9 - upsample\n    u9 = upsample_block(u8, f1, 64)\n\n    # outputs\n    outputs = layers.Conv2D(1, 1, padding=\"same\", activation = \"sigmoid\")(u9)\n\n    # unet model with Keras Functional API\n    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n\n    return unet_model","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:12.396115Z","iopub.execute_input":"2023-04-27T03:22:12.396740Z","iopub.status.idle":"2023-04-27T03:22:12.411237Z","shell.execute_reply.started":"2023-04-27T03:22:12.396705Z","shell.execute_reply":"2023-04-27T03:22:12.410242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unet_model = build_unet_model()\n#unet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:12.412346Z","iopub.execute_input":"2023-04-27T03:22:12.413099Z","iopub.status.idle":"2023-04-27T03:22:13.109557Z","shell.execute_reply.started":"2023-04-27T03:22:12.413062Z","shell.execute_reply":"2023-04-27T03:22:13.108529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tf.keras.utils.plot_model(unet_model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:13.110976Z","iopub.execute_input":"2023-04-27T03:22:13.111344Z","iopub.status.idle":"2023-04-27T03:22:13.117221Z","shell.execute_reply.started":"2023-04-27T03:22:13.111304Z","shell.execute_reply":"2023-04-27T03:22:13.115981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\nK.clear_session()\n\ntversky_loss_fn = LossFunctions.tversky_loss\nmodel_powerline = VGGUnet(image_size = 256)\nmetrics = [\"accuracy\", \n           tf.keras.metrics.AUC(), \n           tf.keras.metrics.SensitivityAtSpecificity(0.5), \n           tf.keras.metrics.SpecificityAtSensitivity(0.5)]\nmodel_powerline.compile(optimizer=Adam(), loss=tversky_loss_fn, metrics=metrics)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:13.118946Z","iopub.execute_input":"2023-04-27T03:22:13.120287Z","iopub.status.idle":"2023-04-27T03:22:13.873892Z","shell.execute_reply.started":"2023-04-27T03:22:13.120241Z","shell.execute_reply":"2023-04-27T03:22:13.872888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 10\nBATCH_SIZE = 8\n\n\nSTEPS_PER_EPOCH = total_samples // BATCH_SIZE\n\nVAL_SUBSPLITS = 5\nVALIDATION_STEPS = test_size // BATCH_SIZE // VAL_SUBSPLITS\n\ntensorboard = TensorBoard(log_dir='logs/semantic_segmentation')\nmodel_history = model_powerline.fit(train_batches,\n                               epochs=NUM_EPOCHS,\n                               steps_per_epoch=STEPS_PER_EPOCH,\n                               validation_steps=VALIDATION_STEPS,\n                               validation_data=validation_batches,\n                               callbacks=[tensorboard])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:22:13.875583Z","iopub.execute_input":"2023-04-27T03:22:13.875972Z","iopub.status.idle":"2023-04-27T04:32:43.059883Z","shell.execute_reply.started":"2023-04-27T03:22:13.875933Z","shell.execute_reply":"2023-04-27T04:32:43.058784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Visualization of predicion**","metadata":{}},{"cell_type":"code","source":"def create_mask(pred_mask, threshold=0.25):\n  pred_mask = tf.cast(pred_mask, dtype=tf.float32)  # convertir a float\n  pred_mask = tf.where(pred_mask > threshold, 1.0, 0.0)  # usar float para el threshold\n  return pred_mask[0]\n\ndef show_predictions(dataset=None, num=1):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model_powerline.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model_powerline.predict(sample_image[tf.newaxis, ...]))])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:32:43.061882Z","iopub.execute_input":"2023-04-27T04:32:43.062288Z","iopub.status.idle":"2023-04-27T04:32:43.070221Z","shell.execute_reply.started":"2023-04-27T04:32:43.062249Z","shell.execute_reply":"2023-04-27T04:32:43.069108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_predictions(test_batches.skip(5), 50)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:32:43.071798Z","iopub.execute_input":"2023-04-27T04:32:43.072183Z","iopub.status.idle":"2023-04-27T04:33:04.002330Z","shell.execute_reply.started":"2023-04-27T04:32:43.072145Z","shell.execute_reply":"2023-04-27T04:33:04.001206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Metricas**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score, jaccard_score\n\n\"\"\" Prediction & Evaluation \"\"\"\nSCORE = []\nthreshold = 0.3\nnum_classes = 2\nclasses = [\n        \"background\", \"power_line\"\n    ]\n\nfor image, mask in test_batches:\n    pred_mask = model_powerline.predict(image)\n    pred_mask = tf.cast(pred_mask, dtype=tf.float32)  # convertir a float\n    pred = tf.where(pred_mask > threshold, 1.0, 0.0)\n\n    labels = list(np.unique(pred))\n    flat_mask = tf.reshape(mask,[-1])\n    flat_pred = tf.reshape(pred, [-1])\n    \n    mask_arr = flat_mask.numpy().flatten()\n    pred_arr = flat_pred.numpy().flatten()\n\n\n    \"\"\" Calculating the metrics values \"\"\"\n    f1_value = f1_score(mask_arr, pred_arr, labels=labels, average=None, zero_division=0)\n    jac_value = jaccard_score(mask_arr, pred_arr, labels=labels, average=None, zero_division=0)\n\n    SCORE.append([f1_value, jac_value])\n\nscore = np.array(SCORE)\nscore = np.mean(score, axis=0)\n\nl = [\"Class\", \"F1\", \"Jaccard\"]\nprint(f\"{l[0]:15s} {l[1]:10s} {l[2]:10s}\")\nprint(\"-\"*35)\n\nfor i in range(num_classes):\n    class_name = classes[i]\n    f1 = score[0, i]\n    jac = score[1, i]\n    dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\n    print(dstr)\n\nprint(\"-\"*35)\nclass_mean = np.mean(score, axis=-1)\nclass_name = \"Mean\"\n\nf1 = class_mean[0]\njac = class_mean[1]\n\ndstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\nprint(dstr)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T04:33:04.003960Z","iopub.execute_input":"2023-04-27T04:33:04.004410Z","iopub.status.idle":"2023-04-27T04:33:40.451002Z","shell.execute_reply.started":"2023-04-27T04:33:04.004374Z","shell.execute_reply":"2023-04-27T04:33:40.449682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Union of both models**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport cv2\nimport numpy as np\n\n# Cargar los pesos pre-entrenados de las dos redes neuronales\n#model_vegetacion = tf.keras.models.load_model(\"modelo_vegetacion.h5\")\n#model_linea_electrica = tf.keras.models.load_model(\"modelo_linea_electrica.h5\")\n\nfor image, mask in test_batches.skip(2).take(1):\n\n    # Realizar la segmentación semántica para obtener las dos máscaras\n    mask_vegetacion = model_veg.predict(image)\n    mask_vegetacion = create_mask(mask_vegetacion, threshold=0.25)\n    mask_linea_electrica = model_powerline.predict(image)\n    mask_linea_electrica = create_mask(mask_linea_electrica, threshold=0.25)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T05:54:50.413590Z","iopub.execute_input":"2023-04-27T05:54:50.414065Z","iopub.status.idle":"2023-04-27T05:54:58.436867Z","shell.execute_reply.started":"2023-04-27T05:54:50.414008Z","shell.execute_reply":"2023-04-27T05:54:58.435874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_mask = np.zeros_like(image.numpy()[0])\ncombined_mask[np.squeeze(mask_vegetacion == 1), :] = (0, 255, 0) # verde para mask_vegetacion\ncombined_mask[np.squeeze(mask_linea_electrica == 1), :] = (128, 128, 128) # gris para mask_linea_electrica\nfinal_image = cv2.addWeighted(image.numpy()[0], 0.5, combined_mask, 0.5, 0)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T05:59:45.870785Z","iopub.execute_input":"2023-04-27T05:59:45.871181Z","iopub.status.idle":"2023-04-27T05:59:45.885970Z","shell.execute_reply.started":"2023-04-27T05:59:45.871145Z","shell.execute_reply":"2023-04-27T05:59:45.884751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(mask_linea_electrica)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T05:59:46.778508Z","iopub.execute_input":"2023-04-27T05:59:46.778824Z","iopub.status.idle":"2023-04-27T05:59:46.788691Z","shell.execute_reply.started":"2023-04-27T05:59:46.778795Z","shell.execute_reply":"2023-04-27T05:59:46.787146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_image = np.array(final_image, dtype=np.uint8)\n\n# Visualizar la imagen resultante\nplt.imshow(final_image)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T05:59:47.403139Z","iopub.execute_input":"2023-04-27T05:59:47.403701Z","iopub.status.idle":"2023-04-27T05:59:47.613352Z","shell.execute_reply.started":"2023-04-27T05:59:47.403671Z","shell.execute_reply":"2023-04-27T05:59:47.612522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(image[0])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T06:17:57.422617Z","iopub.execute_input":"2023-04-27T06:17:57.423557Z","iopub.status.idle":"2023-04-27T06:17:57.787324Z","shell.execute_reply.started":"2023-04-27T06:17:57.423504Z","shell.execute_reply":"2023-04-27T06:17:57.786128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Crear una máscara para los píxeles que son verdes o grises\nmask_combined = np.logical_and(mask_vegetacion, mask_linea_electrica)\nmask_combined = np.squeeze(mask_combined)\n\n# Convertir la imagen a un array de numpy y cambiar el tipo de dato a uint8\nfinal_image = np.array(final_image, dtype=np.uint8)\n\n# Aplicar la máscara a la imagen original\nfinal_image[mask_combined, :] = [255, 0, 0]  # Rojo: [R, G, B]\n\n# Visualizar la imagen resultante\nplt.imshow(final_image)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T05:59:50.164453Z","iopub.execute_input":"2023-04-27T05:59:50.164812Z","iopub.status.idle":"2023-04-27T05:59:50.372983Z","shell.execute_reply.started":"2023-04-27T05:59:50.164778Z","shell.execute_reply":"2023-04-27T05:59:50.371980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_mask(pred_mask, threshold=0.25):\n  pred_mask = tf.cast(pred_mask, dtype=tf.float32)  # convertir a float\n  pred_mask = tf.where(pred_mask > threshold, 1.0, 0.0)  # usar float para el threshold\n  return pred_mask[0]\n\ndef show_predictions(dataset=None, num=1):\n    if dataset:\n        for image, mask in dataset.take(num):\n            pred_mask_power = model_powerline.predict(image)\n            pred_mask_veg = model_veg.predict(image)\n\n            display([image[0], create_mask(pred_mask_power)])\n  ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T05:53:58.914672Z","iopub.execute_input":"2023-04-27T05:53:58.915295Z","iopub.status.idle":"2023-04-27T05:53:58.922308Z","shell.execute_reply.started":"2023-04-27T05:53:58.915253Z","shell.execute_reply":"2023-04-27T05:53:58.921210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_predictions(test_batches.skip(2), 1)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T05:54:29.977890Z","iopub.execute_input":"2023-04-27T05:54:29.978361Z","iopub.status.idle":"2023-04-27T05:54:40.226446Z","shell.execute_reply.started":"2023-04-27T05:54:29.978311Z","shell.execute_reply":"2023-04-27T05:54:40.225231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_powerline.save('power_line_trained_model.h5')\nmodel_veg.save('veg_trained_model.h5')\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T06:12:14.756722Z","iopub.execute_input":"2023-04-27T06:12:14.757178Z","iopub.status.idle":"2023-04-27T06:12:16.820548Z","shell.execute_reply.started":"2023-04-27T06:12:14.757138Z","shell.execute_reply":"2023-04-27T06:12:16.819131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Register custom loss function\ncustom_objects = {'tversky_loss': LossFunctions.tversky_loss}\n\n# Load model with registered custom objects\nmodel_veg = tf.keras.models.load_model('veg_trained_model.h5', custom_objects=custom_objects)\n\n# Use the model for prediction\npredictions = model_veg.predict(image)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T06:15:35.983290Z","iopub.execute_input":"2023-04-27T06:15:35.984031Z","iopub.status.idle":"2023-04-27T06:15:38.792776Z","shell.execute_reply.started":"2023-04-27T06:15:35.983991Z","shell.execute_reply":"2023-04-27T06:15:38.791816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nmodel_powerline = load_model('power_line_trained_model.h5')\npredictions = model_powerline.predict(image)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T06:12:44.776460Z","iopub.execute_input":"2023-04-27T06:12:44.776988Z","iopub.status.idle":"2023-04-27T06:12:46.060535Z","shell.execute_reply.started":"2023-04-27T06:12:44.776936Z","shell.execute_reply":"2023-04-27T06:12:46.059079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}